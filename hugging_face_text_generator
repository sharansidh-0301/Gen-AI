#using Pipeline

from transformers import pipeline

# Load text-generation pipeline with GPT-2
generator = pipeline(
    "text-generation",
    model="gpt2"
)

# User input
prompt = input("Enter your prompt: ")

# Generate text
output = generator(
    prompt,
    max_length=100,
    num_return_sequences=1
)

print("\nGenerated Text:\n")
print(output[0]["generated_text"])

#Manual

from transformers import AutoTokenizer, AutoModelForCausalLM

# Load tokenizer and model
tokenizer = AutoTokenizer.from_pretrained("gpt2")
model = AutoModelForCausalLM.from_pretrained("gpt2")

prompt = input("Enter your prompt: ")

# Tokenize input
inputs = tokenizer(prompt, return_tensors="pt")

# Generate text
outputs = model.generate(
    inputs["input_ids"],
    max_length=100,
    do_sample=True,
    temperature=0.7
)

# Decode output
generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)

print("\nGenerated Text:\n")
print(generated_text)
